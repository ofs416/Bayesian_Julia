{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSV, DelimitedFiles\n",
    "using Plots\n",
    "using Zygote\n",
    "using ForwardDiff\n",
    "\n",
    "X = [ones(1000, 1) readdlm(\"X.txt\", ' ', Float64)] \n",
    "y = readdlm(\"y.txt\", ' ', Int64) .* 2 .- 1\n",
    "\n",
    "scatter(X[:, 2], X[:, 3], markercolor=y.+2, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Model using:\n",
    "\n",
    "$$ y = w^T X + b + \\epsilon_n$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$ \\epsilon_n \\sim \\mathcal{N}(0, \\sigma^2) $$\n",
    "\n",
    "by adding a third dimension to X, as done above, we can use:\n",
    "\n",
    "$$ y = w^T \\tilde{X} + \\epsilon_n$$\n",
    "\n",
    "Ultimately we have:\n",
    "\n",
    "$$ p(y_n | \\tilde{\\mathbf{x}}_n, \\mathbf{\\theta}) = \\mathcal{N}(y_n | \\mathbf{w}^t \\tilde{\\mathbf{x}}_n, \\sigma^2) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "$$\\begin{align*}\n",
    " p(y_n | \\tilde{\\mathbf{x}}_n, \\mathbf{\\theta}) &= (1 +y_n) \\sigma (\\mathbf{w}^T \\tilde{\\mathbf{x}}_n) + (1 - y_n) (1 - \\sigma (\\mathbf{w}^T \\tilde{\\mathbf{x}}_n))\\\\\n",
    "&= (1 + y_n) \\sigma (\\mathbf{w}^T \\tilde{\\mathbf{x}}_n) + (1 - y_n)  \\sigma (-\\mathbf{w}^T \\tilde{\\mathbf{x}}_n)\\\\\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(X) = 1 / (1 + exp(-clamp.(X, -10_000, 10_000)))\n",
    "\n",
    "likelihood(X, y, w) = logistic.(-y .* (X * w))\n",
    "\n",
    "log_loss(X, y, w) = sum(log.(likelihood(X, y, w))) / size(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = randn(3, 1)\n",
    "predictions = logistic.(X * w)\n",
    "\n",
    "println(log_loss(X, y, w))\n",
    "scatter(X[:, 2], X[:, 3], markercolor=y.+2)\n",
    "contour!(-3:0.01:3, -3:0.01:3, \n",
    "        (x, y) -> logistic.([1 x y] * w)[1], \n",
    "        levels=[0.25,0.5,0.75], c=[0,:black,1], lw=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient ascent\n",
    "function train(Input, labels; lr=0.01, epochs=500)\n",
    "    n_features = size(Input, 2)\n",
    "    params = randn(n_features, 1) .* 0.1 / (1 * n_features) # Initialize weights\n",
    "    println(\"Initial log likelihood = $(log_loss(Input, labels, params))\")\n",
    "    \n",
    "    for i in 1:epochs\n",
    "        lr = lr * 0.9999  # Decrease learning rate\n",
    "        # Compute gradient using Zygote\n",
    "        grad = Zygote.gradient(params -> log_loss(Input, labels, params), params)[1]\n",
    "\n",
    "        # Update weights (ascent because we're maximizing)\n",
    "        params .+= lr * grad\n",
    "\n",
    "        # Print progress every 100 epochs\n",
    "        if i % 100 == 0\n",
    "            loss = log_loss(Input, labels, params)\n",
    "            println(\"Epoch $i: log likelihood = $loss\")\n",
    "            \n",
    "            \n",
    "        end\n",
    "    end\n",
    "    println(\"Training complete.\")\n",
    "    println(\"Final log likelihood = $(log_loss(Input, labels, params))\")\n",
    "    return params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt = train(X, y)\n",
    "pred = logistic.(X * w_opt)\n",
    "\n",
    "scatter(X[:, 2], X[:, 3], markercolor=y.+2)\n",
    "contour!(-3:0.01:3, -3:0.01:3, \n",
    "        (x, y) -> logistic.([1 x y] * w_opt)[1], \n",
    "        levels=[0.25,0.5,0.75], c=[0,:black,1], lw=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Functions\n",
    "\n",
    "Expanding on the above result, we look at other basis functions that better capture non-linearities further from the origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_gaussian_basis_functions(l, X, Z)\n",
    "    X2 = sum(X.^2, dims=2)\n",
    "    Z2 = sum(Z.^2, dims=2)\n",
    "    ones_Z = ones(size(Z, 1))\n",
    "    ones_X = ones(size(X, 1))\n",
    "    r2 = X2 * ones_Z' .- 2 * (X * Z') .+ ones_X * Z2'\n",
    "    return exp.(-0.5 / l^2 .* r2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_basis = evaluate_gaussian_basis_functions(1, X, X)\n",
    "w_opt_basis = train(X_basis, y)\n",
    "predictions = logistic.(X_basis * w_opt_basis)\n",
    "\n",
    "scatter(X[:, 2], X[:, 3], markercolor=y.+2)\n",
    "\n",
    "x_range = range(-3, 3, length=500)\n",
    "y_range = range(-3, 3, length=500)\n",
    "points = hcat(repeat(x_range, inner=length(y_range)), repeat(y_range, outer=length(x_range)))\n",
    "points_basis = evaluate_gaussian_basis_functions(1, [ones(500^2) points], X)\n",
    "points_predictions = logistic.(points_basis * w_opt_basis)\n",
    "points_predictions_mesh = reshape(points_predictions, 500, 500)\n",
    "contour!(x_range, y_range, points_predictions_mesh, levels=[0.25,0.5,0.75], c=[0,:black,1], lw=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
